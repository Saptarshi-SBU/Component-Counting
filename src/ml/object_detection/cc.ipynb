{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Object Detection Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\"> To begin with, the snippet below shows how to use exisiting resnet50 model to perform object detection on custom images. Below is example of REST API calls to fetch images from my personal media library https://github.com/Saptarshi-SBU/APIserver</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get('http://10.2.59.13:4040/api/v1/listphotos')\n",
    "data = response.content\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_data = json.loads(data)\n",
    "albums_uuid = []\n",
    "for kv in j_data\n",
    "    albums_uuid.append(kv['value']['uuid'])\n",
    "print (albums_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Content-Type': 'image/jpg', 'Content-Length': '80698', 'Server': 'Werkzeug/0.16.0 Python/2.7.5', 'Date': 'Mon, 11 Jan 2021 02:07:15 GMT'}\n"
     ]
    }
   ],
   "source": [
    "payload = {'img' : albums_uuid[0]}\n",
    "response = requests.get('http://10.2.59.13:4040/api/v1/scaledphoto', params=payload)\n",
    "print(response)\n",
    "print (response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Installation\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\">Next, lets use conda to install pytorch on the system. I had a lot of trouble installing opencv with python3.7, there seems to be an issue with conda. Follow the following steps:</p>\n",
    "<p style=\"font-family: times, serif; font-size:13pt; font-style:italic\">\n",
    " <br> 1. conda create -n py36 python=3.6</br>\n",
    " <br> 2. conda activate py36 </br>\n",
    " <br> 3. conda install pytorch torchvision torchaudio cudatoolkit=10.2 opencv -c pytorch </br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# get the pretrained model from torchvision.models\n",
    "# Note: pretrained=True will get the pretrained weights for the model.\n",
    "# model.eval() to use the model for inference\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Class labels from official PyTorch documentation for the pretrained model\n",
    "# Note that there are some N/A's \n",
    "# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "# we will use the same list for this notebook\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "\n",
    "def get_prediction(img_path, threshold):\n",
    "  \"\"\"\n",
    "  get_prediction\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - threshold - threshold value for prediction score\n",
    "    method:\n",
    "      - Image is obtained from the image path\n",
    "      - the image is converted to image tensor using PyTorch's Transforms\n",
    "      - image is passed through the model to get the predictions\n",
    "      - class, box coordinates are obtained, but only prediction score > threshold\n",
    "        are chosen.\n",
    "    \n",
    "  \"\"\"\n",
    "  img = Image.open(img_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  pred = model([img])\n",
    "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from io import BytesIO\n",
    "\n",
    "def object_detection_api(img_path, threshold=0.5):\n",
    "  \"\"\"\n",
    "  object_detection_api\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "    method:\n",
    "      - prediction is obtained from get_prediction method\n",
    "      - for each prediction, bounding box is drawn and text is written \n",
    "        with opencv\n",
    "      - the final image is displayed\n",
    "  \"\"\"\n",
    "  boxes, pred_cls = get_prediction(img_path, threshold)\n",
    "  source_img = Image.open(img_path).convert(\"RGB\")\n",
    "  draw = ImageDraw.Draw(source_img)\n",
    "  out_file = \"torch.jpg\"\n",
    "  for i in range(len(boxes)):\n",
    "    draw.rectangle(boxes[i], fill=None, outline=\"yellow\", width=5)\n",
    "    draw.text(boxes[i][0], pred_cls[i], font=ImageFont.truetype(\"/usr/share/fonts/gnu-free/FreeMono.ttf\", 32), fill=\"red\", width=32)\n",
    "  source_img.save(out_file) \n",
    "  im = Image.open(out_file)\n",
    "  im.show()  \n",
    "  print (boxes, pred_cls)\n",
    "  return pred_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Inference and Labeling\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\">\n",
    "Now that we had our object detection api ready, we can run inference on a sample image\n",
    "using the above object detection API</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'img' : albums_uuid[0]}\n",
    "response = requests.get('http://10.2.59.13:4040/api/v1/scaledphoto', params=payload)\n",
    "file_jpgdata = BytesIO(response.content)\n",
    "object_detection_api(file_jpgdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent Labelling using DASK\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\">\n",
    "Next Below is a sample to run object detection and perform labeling concurrently on a batch of images\n",
    "python big data framework DASK. Dask is a work scheduler which acts by paritioning dataset and assign\n",
    "workers to a partition. Since we have thousands of images, the below few lines of code helps us achieve\n",
    "the required throughput</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(49.699875, 61.867325), (527.6324, 750.3205)]] ['person']\n",
      "b'\"person\"'\n",
      "[[(310.35916, 18.110456), (665.9717, 459.42868)], [(310.77026, 609.9912), (435.44623, 711.0056)], [(10.206599, 444.7967), (1008.0, 756.0)], [(177.51633, 490.07703), (314.01196, 572.94586)], [(557.04407, 589.51434), (678.3497, 686.18567)], [(588.5785, 521.7588), (689.3299, 600.59076)], [(142.30682, 506.6534), (251.05882, 545.9602)], [(433.95364, 623.8414), (554.6413, 723.4723)], [(506.91428, 490.52795), (559.3247, 527.59546)]] ['person', 'bowl', 'dining table', 'bowl', 'bowl', 'bowl', 'spoon', 'bowl', 'donut']\n",
      "b'\"person bowl dining table bowl bowl bowl spoon bowl donut\"'\n",
      "[[(164.95193, 247.10779), (583.1691, 999.88477)], [(0.0, 1.0117171), (86.66291, 268.19583)], [(619.72235, 686.3288), (728.5662, 811.3008)], [(168.11859, 3.3667922), (628.5089, 186.56113)], [(39.11149, 34.989086), (715.9636, 874.48486)]] ['person', 'person', 'remote', 'person', 'bed']\n",
      "[[(15.77534, 152.84142), (731.1579, 871.7225)], [(0.0, 525.70044), (71.41812, 707.62177)], [(0.90724236, 447.66003), (80.13275, 996.7512)], [(48.616222, 302.60098), (702.27155, 983.65295)], [(0.0, 497.40982), (77.30299, 708.20294)]] ['person', 'handbag', 'person', 'teddy bear', 'backpack']\n",
      "[[(53.766148, 106.498184), (277.0302, 320.0417)], [(10.335727, 71.42913), (288.0, 512.0)]] ['cake', 'cake']\n",
      "b'\"person person remote person bed\"'\n",
      "b'\"person handbag person teddy bear backpack\"'\n",
      "b'\"cake cake\"'\n",
      "[[(345.84457, 74.08648), (685.62585, 491.55603)], [(130.27419, 470.57535), (223.97435, 657.57477)], [(385.19586, 628.5778), (499.89813, 722.7171)], [(41.430637, 465.57), (940.08167, 751.6261)], [(271.12427, 521.15027), (370.48145, 599.54004)], [(612.3191, 608.4947), (722.5385, 697.02155)], [(630.5377, 545.3821), (725.40985, 612.88293)], [(500.54175, 636.5083), (611.82635, 732.9879)], [(221.33495, 539.7456), (308.06082, 572.87445)], [(549.3675, 517.5963), (597.21387, 551.37427)], [(208.1732, 599.26984), (396.88654, 734.26196)]] ['person', 'cup', 'bowl', 'dining table', 'bowl', 'bowl', 'bowl', 'bowl', 'spoon', 'donut', 'bowl']\n",
      "b'\"person cup bowl dining table bowl bowl bowl bowl spoon donut bowl\"'\n",
      "[[(171.0009, 204.86127), (595.1045, 1008.0)], [(627.761, 671.2423), (734.7699, 799.8006)]] ['person', 'remote']\n",
      "[[(77.84331, 193.25845), (212.39871, 416.8576)], [(29.963253, 197.60886), (187.1338, 478.5768)], [(4.93669, 256.7009), (110.74081, 492.07187)], [(104.92568, 352.49915), (266.19418, 508.78854)], [(2.8807003, 210.36009), (126.975494, 491.17883)]] ['person', 'person', 'dog', 'chair', 'person']\n",
      "b'\"person remote\"'\n",
      "b'\"person person dog chair person\"'\n",
      "[[(359.86816, 192.79099), (696.1721, 475.30453)], [(2.3593278, 54.17147), (756.0, 797.3958)], [(213.15076, 521.11584), (666.2907, 929.9445)], [(43.84253, 311.75247), (395.1804, 652.2862)], [(0.0, 461.6721), (756.0, 995.3065)]] ['bowl', 'dining table', 'sandwich', 'sandwich', 'dining table']\n",
      "[[(286.2882, 74.72214), (794.0172, 560.2973)], [(137.37782, 476.39645), (231.77942, 659.7749)], [(20.150194, 478.36606), (1008.0, 756.0)], [(393.26987, 630.90283), (505.09708, 725.8982)], [(634.5279, 550.1176), (731.5702, 617.0509)], [(273.8381, 526.188), (376.82297, 601.4822)], [(616.106, 610.11017), (726.9859, 698.7187)], [(505.4633, 640.5124), (616.6173, 734.6452)], [(230.39523, 540.46844), (313.06656, 577.0422)], [(553.7544, 519.41614), (603.35114, 554.6045)], [(916.3369, 518.3754), (996.0413, 617.48114)], [(14.122341, 481.49554), (510.90088, 754.2959)], [(213.73161, 602.49725), (401.07843, 735.9756)], [(538.2458, 514.49805), (574.3849, 545.01514)], [(453.78275, 525.89966), (549.48816, 603.663)], [(363.83755, 435.66333), (452.40668, 556.8665)]] ['person', 'cup', 'dining table', 'bowl', 'bowl', 'bowl', 'bowl', 'bowl', 'spoon', 'donut', 'spoon', 'dining table', 'bowl', 'donut', 'cake', 'spoon']\n",
      "b'\"bowl dining table sandwich sandwich dining table\"'\n",
      "b'\"person cup dining table bowl bowl bowl bowl bowl spoon donut spoon dining table bowl donut cake spoon\"'\n",
      "[[(88.81869, 146.43556), (208.50182, 370.37427)], [(0.0, 47.519623), (159.36635, 438.51462)], [(195.43083, 306.90433), (212.71423, 356.05704)], [(119.47291, 318.17004), (157.7138, 355.2082)], [(84.41955, 312.19522), (259.66333, 488.48257)], [(40.06951, 390.6917), (282.3098, 507.65356)]] ['person', 'person', 'chair', 'chair', 'chair', 'dining table']\n",
      "b'\"person person chair chair chair dining table\"'\n",
      "[[(0.0, 115.69156), (730.2508, 1008.0)], [(692.4701, 575.4424), (755.2716, 698.6386)]] ['teddy bear', 'person']\n",
      "[[(443.55972, 292.10715), (964.7983, 682.3738)], [(531.5996, 613.0019), (593.2036, 661.84534)], [(3.3872807, 289.41293), (805.1383, 717.28467)], [(64.31218, 89.220825), (87.8082, 139.35905)], [(182.85965, 520.55817), (996.0342, 740.2227)], [(531.581, 555.393), (588.86694, 593.21594)], [(562.6624, 726.06934), (744.2533, 755.32635)], [(704.9729, 622.7755), (768.73517, 668.9564)], [(578.2793, 629.52344), (641.8613, 683.5086)], [(293.54288, 683.43005), (327.10785, 755.1258)], [(23.481565, 310.15714), (278.3535, 503.73984)], [(575.7154, 614.55035), (621.7895, 637.59015)], [(421.17813, 596.0107), (955.8564, 749.024)], [(221.9776, 642.61993), (412.53583, 750.37866)], [(294.0573, 684.48364), (314.9796, 741.0049)], [(471.20163, 585.99243), (565.02966, 640.9333)]] ['person', 'bowl', 'person', 'clock', 'dining table', 'bowl', 'bowl', 'bowl', 'bowl', 'fork', 'dog', 'spoon', 'dining table', 'bowl', 'fork', 'bowl']\n",
      "b'\"teddy bear person\"'\n",
      "[[(58.326786, 224.57033), (698.3446, 817.2485)], [(0.0, 63.655346), (756.0, 876.3661)]] ['pizza', 'oven']\n",
      "b'\"pizza oven\"'\n",
      "b'\"person bowl person clock dining table bowl bowl bowl bowl fork dog spoon dining table bowl fork bowl\"'\n",
      "[[(24.024595, 267.03403), (714.09564, 956.1118)], [(344.23035, 434.95975), (635.22894, 497.0463)]] ['teddy bear', 'toothbrush']\n",
      "b'\"teddy bear toothbrush\"'\n",
      "[[(425.5325, 191.4351), (580.15375, 353.48172)], [(11.062481, 0.0), (741.9701, 1008.0)], [(342.35834, 390.69528), (474.95026, 446.19247)], [(190.64221, 757.64514), (366.29028, 943.7907)], [(71.75331, 379.8745), (157.18587, 457.75574)], [(491.1847, 352.90063), (662.44147, 515.67004)], [(489.4329, 510.18164), (654.84546, 671.416)], [(231.07114, 143.80595), (390.44235, 287.37735)], [(329.25906, 442.3096), (463.36838, 486.43823)], [(673.80786, 5.9365497), (755.66315, 260.09384)], [(330.4257, 330.85535), (422.27643, 447.27277)], [(52.293125, 564.13556), (172.59262, 661.5809)], [(99.73861, 342.05573), (190.54723, 437.49036)], [(208.80731, 322.73138), (342.58365, 417.79294)]] ['bowl', 'dining table', 'knife', 'bowl', 'carrot', 'bowl', 'bowl', 'bowl', 'knife', 'bowl', 'knife', 'orange', 'carrot', 'cake']\n",
      "b'\"bowl dining table knife bowl carrot bowl bowl bowl knife bowl knife orange carrot cake\"'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import dask\n",
    "import dask.bag as db\n",
    "from io import BytesIO\n",
    "\n",
    "API_HOST = '10.2.59.13'\n",
    "LISTPHOTOS_API = 'http://{}:4040/api/v1/listphotos'.format(API_HOST)\n",
    "GETPHOTO_API = 'http://{}:4040/api/v1/scaledphoto'.format(API_HOST)\n",
    "LABELPHOTO_API = 'http://{}:4040/api/v1/label'.format(API_HOST)\n",
    "\n",
    "def fetch_all_images():\n",
    "    albums_uuid = []\n",
    "    response = requests.get(LISTPHOTOS_URL)\n",
    "    data = response.content\n",
    "    j_data = json.loads(data)\n",
    "    for kv in j_data:\n",
    "        albums_uuid.append(kv['value']['uuid'])\n",
    "    return albums_uuid\n",
    "\n",
    "def label_image(img_uuid):\n",
    "    response = requests.get(GETPHOTO_URL, params={'img':img_uuid})\n",
    "    jpgdata = BytesIO(response.content)\n",
    "    pred_cls = object_detection_api(jpgdata)\n",
    "    labels = ' '.join(pred_cls)\n",
    "    response = requests.post(LABELPHOTO_URL, data={'img':img_uuid, 'labels':labels})\n",
    "    response = requests.get(LABELPHOTO_URL, params={'img':img_uuid})\n",
    "    print (response.content)\n",
    "\n",
    "#fetch_all_images()\n",
    "b = db.from_sequence(albums_uuid[0:16], npartitions=4).map(label_image)\n",
    "r = b.compute()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets, DataLoaders\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\">\n",
    "Next Lets focus on building a pytorch based dataset for our Component Counting project\n",
    "In order to train a PyTorch neural network you must write code to read training data into memory,\n",
    "convert the data to PyTorch tensors, and serve the data up in batches. </p>\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\">\n",
    "The below code snippet creates a PyTorch based DataSet Class for our PID Dataset.\n",
    "Images are annotated using the labelImg too. Next they are converted to a DataFrame using the below listed recipes.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:italic\">\n",
    "<br>pascal_voc_xml_to_csv method : Prepare a DataFrame Object from the xml files</br>\n",
    "<br>CustomImageDataset class : Prepare a PyTorch Dataset Object from the DataFrame</br>\n",
    "<br>CustomImageDataLoader class : Prepare a PyTorch DataLoader which is fed to the Model for training</br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def pascal_voc_xml_to_csv(data_dir):\n",
    "    '''\n",
    "      The recipe is used to convert images annotations\n",
    "      in pascal voc format to the csv format.\n",
    "    '''\n",
    "    xml_list = []\n",
    "    counter = 0\n",
    "    for xml_file in glob(data_dir + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        filename = root.find('filename').text\n",
    "        size = root.find('size')\n",
    "        width = size.find('width').text\n",
    "        height = size.find('height').text\n",
    "        for member in root.findall('object'):\n",
    "            box = member.find('bndbox')\n",
    "            label = member.find('name').text\n",
    "            row = (filename, width, height, label, int(float(box.find('xmin').text)), int(float(box.find('ymin').text)),\n",
    "               int(float(box.find('xmax').text)), int(float(box.find('ymax').text)), counter)\n",
    "            xml_list.append(row)\n",
    "        counter += 1\n",
    "        column_names = ['filename', 'width', 'height', 'label', 'xmin', 'ymin', 'xmax', 'ymax', 'image_id']\n",
    "        xml_df = pd.DataFrame(xml_list, columns=column_names)\n",
    "        print (xml_df)\n",
    "    #xml_df.to_csv('xml2csv.csv')\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   filename width height       label  xmin  ymin  xmax  ymax  \\\n",
      "0   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   540   267   563   284   \n",
      "1   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   472   301   493   314   \n",
      "2   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   410   418   428   436   \n",
      "3   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   375   247   400   268   \n",
      "4   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   831   401   846   422   \n",
      "5   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   814   445   832   462   \n",
      "6   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   837   444   858   463   \n",
      "7   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   829   488   846   507   \n",
      "8   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   329   315   344   332   \n",
      "9   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   330   413   345   431   \n",
      "10  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   326   300   344   312   \n",
      "11  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   324   580   344   601   \n",
      "12  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   325   633   344   652   \n",
      "13  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   324   618   344   632   \n",
      "14  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   326   732   346   750   \n",
      "15  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   615   849   631   870   \n",
      "16  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   622   834   641   854   \n",
      "17  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   367   981   394  1004   \n",
      "18  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   615  1002   632  1019   \n",
      "19  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   364  1083   393  1106   \n",
      "20  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   335  1135   356  1157   \n",
      "21  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   173  1118   189  1136   \n",
      "22  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   128  1114   145  1137   \n",
      "23  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   117  1076   137  1091   \n",
      "24  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   164  1077   179  1088   \n",
      "25  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   408   737   432   752   \n",
      "26  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   469   619   491   632   \n",
      "27  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   636   758   657   778   \n",
      "28  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   828   802   844   829   \n",
      "29  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   829   716   845   741   \n",
      "\n",
      "    image_id  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "5          0  \n",
      "6          0  \n",
      "7          0  \n",
      "8          0  \n",
      "9          0  \n",
      "10         0  \n",
      "11         0  \n",
      "12         0  \n",
      "13         0  \n",
      "14         0  \n",
      "15         0  \n",
      "16         0  \n",
      "17         0  \n",
      "18         0  \n",
      "19         0  \n",
      "20         0  \n",
      "21         0  \n",
      "22         0  \n",
      "23         0  \n",
      "24         0  \n",
      "25         0  \n",
      "26         0  \n",
      "27         0  \n",
      "28         0  \n",
      "29         0  \n",
      "                   filename width height       label  xmin  ymin  xmax  ymax  \\\n",
      "0   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   540   267   563   284   \n",
      "1   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   472   301   493   314   \n",
      "2   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   410   418   428   436   \n",
      "3   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   375   247   400   268   \n",
      "4   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   831   401   846   422   \n",
      "5   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   814   445   832   462   \n",
      "6   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   837   444   858   463   \n",
      "7   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   829   488   846   507   \n",
      "8   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   329   315   344   332   \n",
      "9   6987_61_1112_REV0_1.jpg  1123   1588  ball valve   330   413   345   431   \n",
      "10  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   326   300   344   312   \n",
      "11  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   324   580   344   601   \n",
      "12  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   325   633   344   652   \n",
      "13  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   324   618   344   632   \n",
      "14  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   326   732   346   750   \n",
      "15  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   615   849   631   870   \n",
      "16  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   622   834   641   854   \n",
      "17  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   367   981   394  1004   \n",
      "18  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   615  1002   632  1019   \n",
      "19  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   364  1083   393  1106   \n",
      "20  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   335  1135   356  1157   \n",
      "21  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   173  1118   189  1136   \n",
      "22  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   128  1114   145  1137   \n",
      "23  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   117  1076   137  1091   \n",
      "24  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   164  1077   179  1088   \n",
      "25  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   408   737   432   752   \n",
      "26  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   469   619   491   632   \n",
      "27  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   636   758   657   778   \n",
      "28  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   828   802   844   829   \n",
      "29  6987_61_1112_REV0_1.jpg  1123   1588  ball valve   829   716   845   741   \n",
      "\n",
      "    image_id  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "5          0  \n",
      "6          0  \n",
      "7          0  \n",
      "8          0  \n",
      "9          0  \n",
      "10         0  \n",
      "11         0  \n",
      "12         0  \n",
      "13         0  \n",
      "14         0  \n",
      "15         0  \n",
      "16         0  \n",
      "17         0  \n",
      "18         0  \n",
      "19         0  \n",
      "20         0  \n",
      "21         0  \n",
      "22         0  \n",
      "23         0  \n",
      "24         0  \n",
      "25         0  \n",
      "26         0  \n",
      "27         0  \n",
      "28         0  \n",
      "29         0  \n",
      "image count : 1\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "0 ([tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         ...,\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         ...,\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         ...,\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])], [{'boxes': tensor([[ 540,  267,  563,  284],\n",
      "        [ 472,  301,  493,  314],\n",
      "        [ 410,  418,  428,  436],\n",
      "        [ 375,  247,  400,  268],\n",
      "        [ 831,  401,  846,  422],\n",
      "        [ 814,  445,  832,  462],\n",
      "        [ 837,  444,  858,  463],\n",
      "        [ 829,  488,  846,  507],\n",
      "        [ 329,  315,  344,  332],\n",
      "        [ 330,  413,  345,  431],\n",
      "        [ 326,  300,  344,  312],\n",
      "        [ 324,  580,  344,  601],\n",
      "        [ 325,  633,  344,  652],\n",
      "        [ 324,  618,  344,  632],\n",
      "        [ 326,  732,  346,  750],\n",
      "        [ 615,  849,  631,  870],\n",
      "        [ 622,  834,  641,  854],\n",
      "        [ 367,  981,  394, 1004],\n",
      "        [ 615, 1002,  632, 1019],\n",
      "        [ 364, 1083,  393, 1106],\n",
      "        [ 335, 1135,  356, 1157],\n",
      "        [ 173, 1118,  189, 1136],\n",
      "        [ 128, 1114,  145, 1137],\n",
      "        [ 117, 1076,  137, 1091],\n",
      "        [ 164, 1077,  179, 1088],\n",
      "        [ 408,  737,  432,  752],\n",
      "        [ 469,  619,  491,  632],\n",
      "        [ 636,  758,  657,  778],\n",
      "        [ 828,  802,  844,  829],\n",
      "        [ 829,  716,  845,  741]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])}])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "        Map Style Dataset\n",
    "        https://pytorch.org/docs/stable/data.html#dataset-types\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.df = pascal_voc_xml_to_csv(data_dir) # dataframe\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), \\\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])   \n",
    "        self.classes = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df['image_id'].unique().tolist())\n",
    "    \n",
    "    def __img2tensor(self, img):\n",
    "        if self.transform:\n",
    "            for t in self.transform.transforms:\n",
    "                img = t(img)\n",
    "        return img\n",
    "    \n",
    "    def __bboxes2tensor(self, bboxes):\n",
    "        return torch.tensor(bboxes).view(-1, 4)\n",
    "    \n",
    "    def __labels2tensor(self, labels):\n",
    "        for label in set(labels):\n",
    "            if label not in self.classes:\n",
    "                self.classes.append(label)\n",
    "        label_intarray = [self.classes.index(label) for label in labels]\n",
    "        return torch.tensor(label_intarray)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            https://pandas.pydata.org/docs/user_guide/dsintro.html\n",
    "            This function implement's Torch Dataset __getitem__\n",
    "        '''\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        targets = {}\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        object_entries = self.df.loc[self.df['image_id'] == idx]\n",
    "        filename = object_entries.iloc[0, 0]\n",
    "        for object_idx, row in object_entries.iterrows():\n",
    "            box = object_entries.iloc[object_idx, 4:8]\n",
    "            boxes.append(box)\n",
    "            label = object_entries.iloc[object_idx, 3]\n",
    "            labels.append(label)\n",
    "        imgpath = '{}/{}'.format(self.data_dir, filename)\n",
    "        img = cv2.imread(imgpath)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.__img2tensor(img)\n",
    "        targets['boxes'] = self.__bboxes2tensor(boxes)\n",
    "        targets['labels'] = self.__labels2tensor(labels)\n",
    "        print (imgpath, type(img))\n",
    "        return img, targets\n",
    "      \n",
    "class CustomImageDataLoader(torch.utils.data.DataLoader):\n",
    "    \n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super().__init__(dataset, collate_fn=CustomImageDataLoader.collate_data, **kwargs)\n",
    "        \n",
    "    @staticmethod    \n",
    "    def collate_data(batch):\n",
    "        images, targets = zip(*batch)\n",
    "        return list(images), list(targets)\n",
    "\n",
    "# Example Usage\n",
    "data_dir='data'\n",
    "pascal_voc_xml_to_csv(data_dir)\n",
    "\n",
    "dataset = CustomImageDataset(data_dir)\n",
    "N = dataset.__len__()\n",
    "print('image count :', N)\n",
    "for i in range(N):\n",
    "    img, targets = dataset.__getitem__(i)\n",
    "    #print (targets['boxes'])\n",
    "        \n",
    "my_ldr = CustomImageDataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for (idx, batch) in enumerate(my_ldr):\n",
    "    print (idx, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Our DataLoader to a Model for Training\n",
    "\n",
    "<p style=\"font-family: times, serif; font-size:14pt; font-style:bold\">\n",
    " Next, lets feed our CustomDataset to a pre-trained model, using our customDataLoader\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(1.6344, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(345.6148, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(2.3462e+08, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "data/6987_61_1112_REV0_1.jpg <class 'torch.Tensor'>\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.train()\n",
    "learning_rate=0.005\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "gamma=0.1\n",
    "lr_step_size=3\n",
    "# Get parameters that have grad turned on (i.e. parameters that should be trained)\n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "# Create an optimizer that uses SGD (stochastic gradient descent) to train the parameters\n",
    "optimizer = torch.optim.SGD(parameters, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "# Create a learning rate scheduler that decreases learning rate by gamma every lr_step_size epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=gamma)\n",
    "loader = CustomImageDataLoader(dataset, batch_size=1, shuffle=True)\n",
    "epochs = 10\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for images, targets in loader:\n",
    "        #print(images[0].shape, targets[0])\n",
    "        loss_dict = model(images, targets)\n",
    "        #print (loss_dict)\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "        print (total_loss)\n",
    "        # Zero any old/existing gradients on the model's parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients for each parameter based on the current loss calculation\n",
    "        total_loss.backward()\n",
    "        # Update model parameters from gradients: param -= learning_rate * param.grad\n",
    "        optimizer.step()\n",
    "    #lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
